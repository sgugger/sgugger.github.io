<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Another data science student's blog - Sylvain Gugger</title><link>/</link><description></description><lastBuildDate>Thu, 26 Apr 2018 17:43:00 -0400</lastBuildDate><item><title>Pointer cache for Language Model</title><link>/pointer-cache-for-language-model.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Thu, 26 Apr 2018 17:43:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-26:/pointer-cache-for-language-model.html</guid><category>Deep Learning</category><category>NLP</category></item><item><title>Recurrent Neural Network</title><link>/recurrent-neural-network.html</link><description>&lt;p class="first last"&gt;In Natural Language Processing, traditional neural networks struggle to properly execute the task we give them. To predict the next work in a sentence for instance, or grasp its meaning to somehow classify it, you need to have a structure that can keeps some memory of the words it saw before. That's why Recurrent Neural Network have been designed to do, and we'll look into them in this article.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Sat, 14 Apr 2018 16:31:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-14:/recurrent-neural-network.html</guid><category>Deep Learning</category><category>NLP</category></item><item><title>The 1cycle policy</title><link>/the-1cycle-policy.html</link><description>&lt;p class="first last"&gt;Properly setting the hyper-parameters of a neural network can be challenging, fortunately, there are some recipe that can help.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Sat, 07 Apr 2018 15:23:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-07:/the-1cycle-policy.html</guid><category>Deep Learning</category><category>SGD</category><category>Learning Rate</category></item><item><title>Convolution in depth</title><link>/convolution-in-depth.html</link><description>&lt;p class="first last"&gt;CNNs (Convolutional Neural Network) are the most powerful networks used in computer vision. Let's see what a convolutional layer is all about, from the definition to the implementation in numpy, even with the back propagation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Thu, 05 Apr 2018 11:03:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-04-05:/convolution-in-depth.html</guid><category>Deep Learning</category><category>Convolution</category></item><item><title>SGD Variants</title><link>/sgd-variants.html</link><description>&lt;p class="first last"&gt;Let's get a rapid overview and implementations of the common variants of SGD.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Thu, 29 Mar 2018 16:35:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-29:/sgd-variants.html</guid><category>SGD</category><category>Deep Learning</category></item><item><title>A simple neural net in numpy</title><link>/a-simple-neural-net-in-numpy.html</link><description>&lt;p class="first last"&gt;Now that we have seen how to build a neural net in pytorch, let's try to take it a step further and try to do the same thing in numpy.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Tue, 20 Mar 2018 16:15:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-20:/a-simple-neural-net-in-numpy.html</guid><category>Neural Net</category><category>Back Propagation</category></item><item><title>How Do You Find A Good Learning Rate</title><link>/how-do-you-find-a-good-learning-rate.html</link><description>&lt;p class="first last"&gt;This is the main hyper-parameter to set when we train a neural net, but how do you determine the best value? Here's a technique to quickly decide on one.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Tue, 20 Mar 2018 16:15:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-20:/how-do-you-find-a-good-learning-rate.html</guid><category>SGD</category><category>Learning Rate</category></item><item><title>A Neural Net In Pytorch</title><link>/a-neural-net-in-pytorch.html</link><description>&lt;p class="first last"&gt;The theory is all really nice, but let's actually build a neural net and train it! We'll see how a simple neural net with one hidden layer can learn to recognize digits very efficiently.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Fri, 16 Mar 2018 10:13:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-16:/a-neural-net-in-pytorch.html</guid><category>Neural net</category><category>Pytorch</category><category>Deep learning</category></item><item><title>What Is Deep Learning?</title><link>/what-is-deep-learning.html</link><description>&lt;p class="first last"&gt;What is deep learning? It's a class of algorithms where you train something called a neural net to complete a specific task. Let's begin with a general overview and we will dig into the details in subsequent articles.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Tue, 13 Mar 2018 17:20:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-13:/what-is-deep-learning.html</guid><category>Neural Net</category><category>SGD</category><category>Deep Learning</category></item><item><title>Why Write A Blog?</title><link>/why-write-a-blog.html</link><description>&lt;p&gt;I've just been accepted to follow the 2018 version of Deep learning - part 2 on &lt;a class="reference external" href="http://fast.ai/"&gt;fast.ai&lt;/a&gt;, and I'm pretty excited about it. As I'm reaching the stage at which this is becoming more
than a hobby and the plan is to switch careers to Data Science, I've taken the â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Gugger</dc:creator><pubDate>Mon, 12 Mar 2018 16:57:00 -0400</pubDate><guid isPermaLink="false">tag:None,2018-03-12:/why-write-a-blog.html</guid></item></channel></rss>